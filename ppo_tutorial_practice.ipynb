{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ppo_tutorial_practice.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMIDYG+iE9bJWTWFuXtSufR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RZvq8WctlkyR"},"source":["# Proximal Policy Optimization (PPO) Tutorial"]},{"cell_type":"markdown","metadata":{"id":"TZSAUgK9l4ZJ"},"source":["## 1. Environment Preparation\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D77XeHmdmYOB"},"source":["### 1.1 Download Packages for BipedalWalker-v3"]},{"cell_type":"code","metadata":{"id":"B6Wj5AJRmVoo"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install colabgymrender==1.0.2\n","!pip install box2d-py\n","!pip install 'gym[Box2D]'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AhbbMMfbnkCD"},"source":["### 1.2 Mount Drive and Set Project Path"]},{"cell_type":"code","metadata":{"id":"mEbRp03dmKR4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","project_root = '/content/drive/My Drive/ppo_tutorial/'\n","sys.path.append(project_root)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3KGagltVqdFw"},"source":["### 1.3 Test the BipedalWalker-v3 Environment "]},{"cell_type":"code","metadata":{"id":"DSOEbSc7nNaB"},"source":["import os\n","import gym\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from colabgymrender.recorder import Recorder\n","\n","env = gym.make('BipedalWalker-v3')\n","s_dim = env.observation_space.shape[0]\n","a_dim = env.action_space.shape[0]\n","print(s_dim)\n","print(a_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsbCyyGdm7tR"},"source":["## 2. Policy Network & Value Network Construction"]},{"cell_type":"markdown","metadata":{"id":"uZrbzM1S0PGg"},"source":["### 2.1 Diagonal Gaussian Distribution Module "]},{"cell_type":"code","metadata":{"id":"R7Xy2egcnB9H"},"source":["#AddBias module\n","class AddBias(nn.Module):\n","    def __init__(self, bias):\n","        super(AddBias, self).__init__()\n","        self._bias = nn.Parameter(bias.unsqueeze(1))\n","    \n","    def forward(self, x):\n","        bias = self._bias.t().view(1, -1)\n","        return x + bias\n","\n","#Gaussian distribution with given mean & std.\n","class FixedNormal(torch.distributions.Normal):\n","    def log_probs(self, x):\n","        return super().log_prob(x).sum(-1)\n","    \n","    def entropy(self):\n","        return super().entropy().sum(-1)\n","\n","    def mode(self):\n","        return self.mean\n","\n","#Diagonal Gaussian module\n","class DiagGaussian(nn.Module):\n","    def __init__(self, inp_dim, out_dim):\n","        super(DiagGaussian, self).__init__()\n","        self.fc_mean = nn.Linear(inp_dim, out_dim)\n","        self.b_logstd = AddBias(torch.zeros(out_dim))\n","    \n","    def forward(self, x):\n","        mean = self.fc_mean(x)\n","        logstd = self.b_logstd(torch.zeros_like(mean))\n","        return FixedNormal(mean, logstd.exp())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDBBOmDe3Unn"},"source":["### 2.2 Policy Network & Value Network Module"]},{"cell_type":"code","metadata":{"id":"xPRvKWq7nZup"},"source":["#Policy Network\n","class PolicyNet(nn.Module):\n","    #Constructor\n","    def __init__(self, s_dim, a_dim):\n","        super(PolicyNet, self).__init__()\n","        #TODO(Lab-1): Policy Network Architecture\n","    \n","    #Forward pass\n","    def forward(self, state, deterministic=False):\n","        feature = self.main(state)\n","        dist = self.dist(feature)\n","\n","        if deterministic:\n","            action = dist.mode()\n","        else:\n","            action = dist.sample()\n","        \n","        return action, dist.log_probs(action)\n","    \n","    #Choose an action (stochastically or deterministically)\n","    def choose_action(self, state, deterministic=False):\n","        feature = self.main(state)\n","        dist = self.dist(feature)\n","\n","        if deterministic:\n","            return dist.mode()\n","\n","        return dist.sample()\n","    \n","    #Evaluate a state-action pair (output log-prob. & entropy)\n","    def evaluate(self, state, action):\n","        feature = self.main(state)\n","        dist = self.dist(feature)\n","        return dist.log_probs(action), dist.entropy()\n","\n","#Value Network\n","class ValueNet(nn.Module):\n","    #Constructor\n","    def __init__(self, s_dim):\n","        super(ValueNet, self).__init__()\n","        #TODO(Lab-2): Value Network Architecture\n","\n","    #Forward pass\n","    def forward(self, state):\n","        return self.main(state)[:, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPZG8h2u7iEY"},"source":["### 2.3 Create Policy Network & Value Network"]},{"cell_type":"code","metadata":{"id":"3bccYpNWnmxB"},"source":["policy_net = PolicyNet(s_dim, a_dim)\n","value_net = ValueNet(s_dim)\n","print(policy_net)\n","print(value_net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiAHu2dtoANa"},"source":["## 3. Environment Runner Construction"]},{"cell_type":"markdown","metadata":{"id":"LIkfaD4GWtGh"},"source":["### 3.1 EnvRunner Class"]},{"cell_type":"code","metadata":{"id":"G5Z0lT_8oKfo"},"source":["class EnvRunner:\n","    #Constructor\n","    def __init__(self, s_dim, a_dim, gamma=0.99, lamb=0.95, max_step=2048, device='cpu'):\n","        self.s_dim = s_dim\n","        self.a_dim = a_dim\n","        self.gamma = gamma\n","        self.lamb = lamb\n","        self.max_step = max_step\n","        self.device = device\n","\n","        #Storages (state, action, value, reward, a_logp)\n","        self.mb_states = np.zeros((self.max_step, self.s_dim), dtype=np.float32)\n","        self.mb_actions = np.zeros((self.max_step, self.a_dim), dtype=np.float32)\n","        self.mb_values = np.zeros((self.max_step,), dtype=np.float32)\n","        self.mb_rewards = np.zeros((self.max_step,), dtype=np.float32)\n","        self.mb_a_logps = np.zeros((self.max_step,), dtype=np.float32)\n","    \n","    #Compute discounted return\n","    def compute_discounted_return(self, rewards, last_value):\n","        #TODO(Lab-3): Compute discounted return\n","\n","        return returns\n","    \n","    #Compute generalized advantage estimation (Optional)\n","    def compute_gae(self, rewards, values, last_value):\n","        advs = np.zeros_like(rewards)\n","        n_step = len(rewards)\n","        last_gae_lam = 0.0\n","\n","        for t in reversed(range(n_step)):\n","            if t == n_step - 1:\n","                next_value = last_value\n","            else:\n","                next_value = values[t+1]\n","\n","            delta = rewards[t] + self.gamma*next_value - values[t]\n","            advs[t] = last_gae_lam = delta + self.gamma*self.lamb*last_gae_lam\n","\n","        return advs + values\n","\n","    #Run an episode using the policy net & value net\n","    def run(self, env, policy_net, value_net):\n","        #TODO(Lab-4): Run an episode to collect data\n","        \n","        #Compute returns\n","        last_value = value_net(\n","            torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)\n","        ).cpu().numpy()\n","\n","        mb_returns = self.compute_discounted_return(self.mb_rewards[:episode_len], last_value)\n","        '''\n","        mb_returns = self.compute_gae(\n","            self.mb_rewards[:episode_len], \n","            self.mb_values[:episode_len],\n","            last_value\n","        )\n","        '''\n","        return self.mb_states[:episode_len], \\\n","                self.mb_actions[:episode_len], \\\n","                self.mb_a_logps[:episode_len], \\\n","                self.mb_values[:episode_len], \\\n","                mb_returns, \\\n","                self.mb_rewards[:episode_len]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8mXvKD2FYlkf"},"source":["### 3.2 Create EnvRunner"]},{"cell_type":"code","metadata":{"id":"osGbTQQpoX3F"},"source":["runner = EnvRunner(s_dim, a_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ntP7LHFocXJ"},"source":["## 4. PPO Algorithm"]},{"cell_type":"markdown","metadata":{"id":"Hgs5NUQGaVPf"},"source":["### 4.1 PPO Class"]},{"cell_type":"code","metadata":{"id":"Jn5EsadHoiQg"},"source":["class PPO:\n","    #Constructor\n","    def __init__(self, policy_net, value_net, lr=1e-4, max_grad_norm=0.5, ent_weight=0.01, clip_val=0.2, sample_n_epoch=4, sample_mb_size=64, device='cpu'):\n","        self.policy_net = policy_net\n","        self.value_net = value_net\n","        self.max_grad_norm = max_grad_norm\n","        self.ent_weight = ent_weight\n","        self.clip_val = clip_val\n","        self.sample_n_epoch = sample_n_epoch\n","        self.sample_mb_size = sample_mb_size\n","        self.device = device\n","        self.opt_polcy = torch.optim.Adam(policy_net.parameters(), lr)\n","        self.opt_value = torch.optim.Adam(value_net.parameters(), lr)\n","    \n","    #Train the policy net & value net using PPO\n","    def train(self, mb_states, mb_actions, mb_old_values, mb_advs, mb_returns, mb_old_a_logps):\n","        #Convert numpy array to tensor\n","        mb_states = torch.from_numpy(mb_states).to(self.device)\n","        mb_actions = torch.from_numpy(mb_actions).to(self.device)\n","        mb_old_values = torch.from_numpy(mb_old_values).to(self.device)\n","        mb_advs = torch.from_numpy(mb_advs).to(self.device)\n","        mb_returns = torch.from_numpy(mb_returns).to(self.device)\n","        mb_old_a_logps = torch.from_numpy(mb_old_a_logps).to(self.device)\n","        episode_length = len(mb_states)\n","        rand_idx = np.arange(episode_length)\n","        sample_n_mb = episode_length // self.sample_mb_size\n","\n","        if sample_n_mb <= 0:\n","            sample_mb_size = episode_length\n","            sample_n_mb = 1\n","        else:\n","            sample_mb_size = self.sample_mb_size\n","\n","        for i in range(self.sample_n_epoch):\n","            np.random.shuffle(rand_idx)\n","\n","            for j in range(sample_n_mb):\n","                #Randomly sample a batch for training\n","                sample_idx = rand_idx[j*sample_mb_size : (j+1)*sample_mb_size]\n","                sample_states = mb_states[sample_idx]\n","                sample_actions = mb_actions[sample_idx]\n","                sample_old_values = mb_old_values[sample_idx]\n","                sample_advs = mb_advs[sample_idx]\n","                sample_returns = mb_returns[sample_idx]\n","                sample_old_a_logps = mb_old_a_logps[sample_idx]\n","\n","                sample_a_logps, sample_ents = self.policy_net.evaluate(sample_states, sample_actions)\n","                sample_values = self.value_net(sample_states)\n","                ent = sample_ents.mean()\n","\n","                #TODO(Lab-5): Compute value loss & policy gradient loss\n","\n","                #Train actor\n","                self.opt_polcy.zero_grad()\n","                pg_loss.backward()\n","                nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.max_grad_norm)\n","                self.opt_polcy.step()\n","\n","                #Train critic\n","                self.opt_value.zero_grad()\n","                v_loss.backward()\n","                nn.utils.clip_grad_norm_(self.value_net.parameters(), self.max_grad_norm)\n","                self.opt_value.step()\n","\n","        return pg_loss.item(), v_loss.item(), ent.item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Npiu_IeAaa3m"},"source":["### 4.2 Create PPO Agent"]},{"cell_type":"code","metadata":{"id":"AcDzxqhOoyzz"},"source":["agent = PPO(policy_net, value_net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gu_Pukpbo1uw"},"source":["## 5. Training and Testing Process"]},{"cell_type":"markdown","metadata":{"id":"XfT0oUo7bNOo"},"source":["### 5.1 Play an Episode for Evaluation"]},{"cell_type":"code","metadata":{"id":"dOn7Ok6TpE85"},"source":["def play(policy_net):\n","    render_env = Recorder(gym.make('BipedalWalker-v3'), project_root + '/video')\n","\n","    with torch.no_grad():\n","        #TODO(Lab-6): Play an episode and evaluate the performance\n","\n","    render_env.play()\n","    render_env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwhxKCl_pp4y"},"source":["play(policy_net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Te32jFabU4v"},"source":["### 5.2 Train the Networks using PPO"]},{"cell_type":"code","metadata":{"id":"2XnleKrKpcbI"},"source":["def train(env, runner, policy_net, value_net, agent, max_episode=5000):\n","    mean_total_reward = 0\n","    mean_length = 0\n","    save_dir = project_root + '/save'\n","\n","    if not os.path.exists(save_dir):\n","        os.mkdir(save_dir)\n","\n","    for i in range(max_episode):\n","        #TODO(Lab-7): Run an episode to collect data and then train the model\n","\n","        #Show the current result & save the model\n","        if i % 200 == 0:\n","            print(\"\\n[{:5d} / {:5d}]\".format(i, max_episode))\n","            print(\"----------------------------------\")\n","            print(\"actor loss = {:.6f}\".format(pg_loss))\n","            print(\"critic loss = {:.6f}\".format(v_loss))\n","            print(\"entropy = {:.6f}\".format(ent))\n","            print(\"mean return = {:.6f}\".format(mean_total_reward / 200))\n","            print(\"mean length = {:.2f}\".format(mean_length / 200))\n","            print(\"\\nSaving the model ... \", end=\"\")\n","            torch.save({\n","                \"it\": i,\n","                \"PolicyNet\": policy_net.state_dict(),\n","                \"ValueNet\": value_net.state_dict()\n","            }, os.path.join(save_dir, \"model.pt\"))\n","            print(\"Done.\")\n","            print()\n","            play(policy_net)\n","            mean_total_reward = 0\n","            mean_length = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LVFcv-pOpujx"},"source":["train(env, runner, policy_net, value_net, agent)\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59tHFUVFbsTR"},"source":["### 5.3 Load the Model and Play"]},{"cell_type":"code","metadata":{"id":"hrqK62jpqF9B"},"source":["save_dir = project_root + '/save'\n","model_path = os.path.join(save_dir, \"model.pt\")\n","\n","if os.path.exists(model_path):\n","    print(\"Loading the model ... \", end=\"\")\n","    checkpoint = torch.load(model_path)\n","    policy_net.load_state_dict(checkpoint[\"PolicyNet\"])\n","    print(\"Done.\")\n","else:\n","    print('ERROR: No model saved')\n","\n","play(policy_net)"],"execution_count":null,"outputs":[]}]}